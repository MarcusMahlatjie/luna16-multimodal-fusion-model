{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · CNN Baseline Model\n",
    "**Purpose** This notebook builds a baseline convolutional neural network (CNN) for classifying lung nodules from the LUNA16 dataset. The aim is not to optimize performance but to establish a reference model that later experiments can improve upon.\n",
    "\n",
    "Key steps include:\n",
    "- Loading preprocessed image patches and labels\n",
    "- Defining a simple CNN architecture using standard layers\n",
    "- Training and evaluating the model on a train/validation split\n",
    "- Reporting baseline accuracy and loss curves\n",
    "\n",
    "This baseline provides a starting point to measure the impact of more advanced architectures, hyperparameter tuning, and the integration of clinical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:36:26.631784Z",
     "iopub.status.busy": "2025-08-27T16:36:26.631554Z",
     "iopub.status.idle": "2025-08-27T16:36:39.940365Z",
     "shell.execute_reply": "2025-08-27T16:36:39.939684Z",
     "shell.execute_reply.started": "2025-08-27T16:36:26.631759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet fvcore iopath pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T16:38:23.150087Z",
     "iopub.status.busy": "2025-08-27T16:38:23.149352Z",
     "iopub.status.idle": "2025-08-27T16:38:23.431070Z",
     "shell.execute_reply": "2025-08-27T16:38:23.430501Z",
     "shell.execute_reply.started": "2025-08-27T16:38:23.150061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.models.video import r3d_18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:39:11.865322Z",
     "iopub.status.busy": "2025-08-27T16:39:11.865044Z",
     "iopub.status.idle": "2025-08-27T16:39:11.879817Z",
     "shell.execute_reply": "2025-08-27T16:39:11.879263Z",
     "shell.execute_reply.started": "2025-08-27T16:39:11.865300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATCH_DATA = Path(\"/kaggle/input/patches/\")\n",
    "PATCH_DIR  = PATCH_DATA / \"patches_64mm\"\n",
    "patch_df   = pd.read_csv(PATCH_DATA / \"patch_index.csv\")\n",
    "profile_df = pd.read_csv(PATCH_DATA / \"synthetic_profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell checks that the patch directory exists and contains the expected 1186 .npy patch files, then shows the first two rows of the patch index DataFrame as a quick preview to confirm the metadata has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:39:14.359521Z",
     "iopub.status.busy": "2025-08-27T16:39:14.359202Z",
     "iopub.status.idle": "2025-08-27T16:39:14.500986Z",
     "shell.execute_reply": "2025-08-27T16:39:14.500399Z",
     "shell.execute_reply.started": "2025-08-27T16:39:14.359499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_file</th>\n",
       "      <th>seriesuid</th>\n",
       "      <th>diam_mm</th>\n",
       "      <th>center_x</th>\n",
       "      <th>center_y</th>\n",
       "      <th>center_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>5.651471</td>\n",
       "      <td>-128.699421</td>\n",
       "      <td>-175.319272</td>\n",
       "      <td>-298.387506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>4.224708</td>\n",
       "      <td>103.783651</td>\n",
       "      <td>-211.925149</td>\n",
       "      <td>-227.121250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          patch_file  \\\n",
       "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...   \n",
       "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...   \n",
       "\n",
       "                                           seriesuid   diam_mm    center_x  \\\n",
       "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  5.651471 -128.699421   \n",
       "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  4.224708  103.783651   \n",
       "\n",
       "     center_y    center_z  \n",
       "0 -175.319272 -298.387506  \n",
       "1 -211.925149 -227.121250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert PATCH_DIR.exists() and len(list(PATCH_DIR.glob(\"*.npy\"))) == 1186\n",
    "display(patch_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build synthetic negative samples to balance the dataset. First, we collect all positive nodule centres per scan, then define a helper (random_bg_coord) that randomly picks a background location in the scan, ensuring it is at least a set distance from any positive nodule. For each patient (seriesuid), one such background coordinate is generated, recorded with label = 0, and stored in a new DataFrame (neg_df). The result is a table of negative examples (non-nodules) that can be combined with the positive samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:39:23.773930Z",
     "iopub.status.busy": "2025-08-27T16:39:23.773288Z",
     "iopub.status.idle": "2025-08-27T16:39:33.710993Z",
     "shell.execute_reply": "2025-08-27T16:39:33.710303Z",
     "shell.execute_reply.started": "2025-08-27T16:39:23.773906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seriesuid</th>\n",
       "      <th>center_z</th>\n",
       "      <th>center_y</th>\n",
       "      <th>center_x</th>\n",
       "      <th>diam_mm</th>\n",
       "      <th>label</th>\n",
       "      <th>patch_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100953483028...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.100953483028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           seriesuid  center_z  center_y  \\\n",
       "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...        32        32   \n",
       "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...        32        32   \n",
       "2  1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...        32        32   \n",
       "3  1.3.6.1.4.1.14519.5.2.1.6279.6001.100953483028...        32        32   \n",
       "4  1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408...        32        32   \n",
       "\n",
       "   center_x  diam_mm  label                                         patch_file  \n",
       "0        32        0      0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  \n",
       "1        32        0      0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...  \n",
       "2        32        0      0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...  \n",
       "3        32        0      0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100953483028...  \n",
       "4        32        0      0  1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centres = {}\n",
    "\n",
    "for _, r in patch_df.iterrows():\n",
    "    centres.setdefault(r.seriesuid, []).append(\n",
    "        np.array([r.center_z, r.center_y, r.center_x])\n",
    "    )\n",
    "\n",
    "def random_bg_coord(scan_shape, pos_list, min_dist_vox=15):\n",
    "    \"\"\"sample a background centre at least min_dist_vox from all positives\"\"\"\n",
    "    for _ in range(1000):\n",
    "        z = random.randint(32, scan_shape[0]-32)\n",
    "        y = random.randint(32, scan_shape[1]-32)\n",
    "        x = random.randint(32, scan_shape[2]-32)\n",
    "        c = np.array([z, y, x])\n",
    "        if all(np.linalg.norm(c - p) >= min_dist_vox for p in pos_list):\n",
    "            return c\n",
    "    return c\n",
    "\n",
    "neg_records = []\n",
    "for suid, rows in patch_df.groupby(\"seriesuid\"):\n",
    "    first_patch = np.load(PATCH_DIR / rows.iloc[0].patch_file)\n",
    "    scan_shape = first_patch.shape\n",
    "    centre = random_bg_coord(scan_shape, centres[suid])\n",
    "    neg_records.append({\n",
    "        \"seriesuid\": suid,\n",
    "        \"center_z\": centre[0], \"center_y\": centre[1], \"center_x\": centre[2],\n",
    "        \"diam_mm\": 0, \"label\": 0, \"patch_file\": f\"{suid}_bg.npy\"\n",
    "    })\n",
    "    \n",
    "neg_df = pd.DataFrame(neg_records)\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:57:16.117554Z",
     "iopub.status.busy": "2025-08-27T16:57:16.117271Z",
     "iopub.status.idle": "2025-08-27T16:57:16.122973Z",
     "shell.execute_reply": "2025-08-27T16:57:16.122402Z",
     "shell.execute_reply.started": "2025-08-27T16:57:16.117534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_64_cube(cube, size=64):\n",
    "    cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    def fix_axis(a, axis, target=size):\n",
    "        s = a.shape[axis]\n",
    "        if s >= target:                         # center-crop\n",
    "            start = (s - target) // 2\n",
    "            sl = [slice(None)]*3\n",
    "            sl[axis] = slice(start, start+target)\n",
    "            return a[tuple(sl)]\n",
    "        else:                                   # pad\n",
    "            before = (target - s)//2\n",
    "            after  = target - s - before\n",
    "            pad = [(0,0)]*3\n",
    "            pad[axis] = (before, after)\n",
    "            return np.pad(a, pad, mode=\"constant\")\n",
    "\n",
    "    cube = fix_axis(cube, 0)\n",
    "    cube = fix_axis(cube, 1)\n",
    "    cube = fix_axis(cube, 2)\n",
    "    return np.ascontiguousarray(cube)           # no negative strides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to define a custom PyTorch Dataset (LunaPatchDS) that combines positive and negative patches, loads cubes, applies augmentations, and returns tensors with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:01:06.353544Z",
     "iopub.status.busy": "2025-08-27T17:01:06.352631Z",
     "iopub.status.idle": "2025-08-27T17:01:06.361031Z",
     "shell.execute_reply": "2025-08-27T17:01:06.360312Z",
     "shell.execute_reply.started": "2025-08-27T17:01:06.353507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LunaPatchDS(Dataset):\n",
    "    def __init__(self, pos_df, neg_df, patch_dir, augment=True):\n",
    "        self.df = pd.concat([pos_df.assign(label=1), neg_df]).sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "        self.patch_dir, self.augment = patch_dir, augment\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _load_cube(self, row):\n",
    "        if row.label == 1:\n",
    "            cube = np.load(self.patch_dir / row.patch_file)\n",
    "        else:\n",
    "            cube = np.random.normal(0, 0.05, (64,64,64)).astype(np.float32)\n",
    "        return to_64_cube(cube)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row  = self.df.iloc[idx]\n",
    "        cube = self._load_cube(row)\n",
    "\n",
    "        if self.augment and row.label == 1:\n",
    "            if random.random() < .5: cube = cube[::-1]\n",
    "            if random.random() < .5: cube = np.rot90(cube, 1, (1,2))\n",
    "            cube = np.ascontiguousarray(cube)\n",
    "\n",
    "        cube  = torch.from_numpy(cube).float().unsqueeze(0)\n",
    "        label = torch.tensor(row.label, dtype=torch.float32)\n",
    "        return cube, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to create an instance of the LunaPatchDS class, providing it with the positive and negative DataFrames as well as the patch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:01:07.750221Z",
     "iopub.status.busy": "2025-08-27T17:01:07.749549Z",
     "iopub.status.idle": "2025-08-27T17:01:07.761140Z",
     "shell.execute_reply": "2025-08-27T17:01:07.760467Z",
     "shell.execute_reply.started": "2025-08-27T17:01:07.750197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = LunaPatchDS(patch_df, neg_df, PATCH_DIR, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to wrap the dataset into a PyTorch DataLoader so that training can be done in shuffled mini-batches with multiprocessing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:01:09.799158Z",
     "iopub.status.busy": "2025-08-27T17:01:09.798514Z",
     "iopub.status.idle": "2025-08-27T17:01:09.803130Z",
     "shell.execute_reply": "2025-08-27T17:01:09.802475Z",
     "shell.execute_reply.started": "2025-08-27T17:01:09.799135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to set up the baseline 3D CNN model for training. First, we check if a GPU is available  as this model development is on kaggle (for its free compute offering) and set the computation device accordingly. We then load the ResNet-18 3D backbone (r3d_18) without pretrained weights and adapt its input layer to accept single-channel CT cubes instead of 3-channel images. Finally, we replace the fully connected head with a small classifier: a linear layer → ReLU activation → dropout for regularization → final linear layer outputting a single value for binary classification. The model is then moved onto the selected device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:01:11.906547Z",
     "iopub.status.busy": "2025-08-27T17:01:11.906246Z",
     "iopub.status.idle": "2025-08-27T17:01:52.530532Z",
     "shell.execute_reply": "2025-08-27T17:01:52.529794Z",
     "shell.execute_reply.started": "2025-08-27T17:01:11.906526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): BasicStem(\n",
       "    (0): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = r3d_18(weights=None)\n",
    "model.stem[0] = nn.Conv3d(\n",
    "    1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    ")\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:02:18.925157Z",
     "iopub.status.busy": "2025-08-27T17:02:18.924861Z",
     "iopub.status.idle": "2025-08-27T17:02:18.997514Z",
     "shell.execute_reply": "2025-08-27T17:02:18.996913Z",
     "shell.execute_reply.started": "2025-08-27T17:02:18.925136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 64, 64, 64]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(DataLoader(train_ds, batch_size=8, num_workers=0)))\n",
    "print(xb.shape, yb.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to train the model with a binary cross-entropy with logits loss, using AdamW optimization and mixed precision (PyTorch autocast + GradScaler) for speed and stability. We track performance each epoch with AUROC from torchmetrics, accumulate average loss, and print both metrics. Whenever the epoch’s AUROC exceeds the previous best, we update best_auc and save the checkpoint (cnn_baseline.pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:02:21.453270Z",
     "iopub.status.busy": "2025-08-27T17:02:21.453032Z",
     "iopub.status.idle": "2025-08-27T17:07:42.270963Z",
     "shell.execute_reply": "2025-08-27T17:07:42.270100Z",
     "shell.execute_reply.started": "2025-08-27T17:02:21.453253Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 | loss 0.1462 | AUROC 0.979\n",
      "  ↳ saved new best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | loss 0.0505 | AUROC 0.994\n",
      "  ↳ saved new best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 | loss 0.0490 | AUROC 0.994\n",
      "  ↳ saved new best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03 | loss 0.0515 | AUROC 0.994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    ^^^self._shutdown_workers()^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^^    ^if w.is_alive():^^\n",
      "^ ^^ ^ ^ \n",
      "AssertionError :  can only test a child process \n",
      "^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    ^^if w.is_alive():^^^\n",
      "^ ^  \n",
      "    File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^^ ^^   ^^ ^  ^^ ^ \n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^  ^ ^ ^  ^  ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^: ^^can only test a child process\n",
      "^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      ": can only test a child process    \n",
      "self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>    \n",
      "if w.is_alive():Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "\n",
      "     self._shutdown_workers()  \n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "     if w.is_alive():\n",
      "      ^  ^Exception ignored in:  ^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^^\n",
      "^Traceback (most recent call last):\n",
      "^^^^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^^    ^^self._shutdown_workers()^^\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "          File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "\n",
      "                       ^   ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^^^ ^^^^ ^^^ ^ ^^^ ^ ^^^^ ^^^^ ^^ ^^^ ^^\n",
      "^ ^AssertionError\n",
      ": ^can only test a child processAssertionError^: \n",
      "can only test a child process^^\n",
      "^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    ^self._shutdown_workers()\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^^    ^^if w.is_alive():\n",
      "^^   ^ ^ ^^ ^^ ^^^^^^^^^\n",
      "^AssertionError^: ^^can only test a child process^\n",
      "^^^\n",
      "Exception ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "       self._shutdown_workers() \n",
      "    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "      Exception ignored in: if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60> \n",
      "  \n",
      " ^ Traceback (most recent call last):\n",
      "^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    ^ self._shutdown_workers()^ \n",
      "^ ^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      " ^^    ^^if w.is_alive():\n",
      "^^^ ^^ ^^ ^^^ ^^ ^^ ^^^ ^^^^^^\n",
      "^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^ ^^ ^^^ ^^^ \n",
      " AssertionError\n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    :  assert self._parent_pid == os.getpid(), 'can only test a child process' can only test a child process\n",
      "\n",
      "      ^Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^ ^\n",
      " ^Traceback (most recent call last):\n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^ ^ ^    ^ self._shutdown_workers() \n",
      "^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^^^^    ^^if w.is_alive():^\n",
      "^^ ^^ ^^ ^^^^^^ ^^^^ ^^^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^AssertionError^: ^can only test a child process^^\n",
      "\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    Exception ignored in: AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      ": \n",
      "  can only test a child processTraceback (most recent call last):\n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "\n",
      "      self._shutdown_workers()   \n",
      " Exception ignored in:    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60> \n",
      "^    ^Traceback (most recent call last):\n",
      "if w.is_alive():^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "\n",
      "    ^ ^self._shutdown_workers() \n",
      "^ ^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      " ^^     ^ if w.is_alive(): ^\n",
      " ^^ ^^^ ^^ ^^ ^^^^ ^^^ ^^^^^^^^^^^^^^\n",
      "^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^\n",
      " ^AssertionError ^:   ^can only test a child process ^  \n",
      "\n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "     assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
      " ^^  ^  ^^  ^^^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError: can only test a child process^\n",
      "^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^^    self._shutdown_workers()^^\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "\n",
      "    AssertionErrorif w.is_alive():: \n",
      "can only test a child process \n",
      "   Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60> \n",
      " Traceback (most recent call last):\n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    ^self._shutdown_workers()^^\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^    ^^if w.is_alive():^\n",
      "^  ^ ^ ^ \n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "     ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^^  ^ ^^ ^^ ^ ^  \n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "      ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^   ^^ ^ ^ ^ ^ ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^: ^can only test a child process^^^\n",
      "^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04 | loss 0.0588 | AUROC 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    self._shutdown_workers()^\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^^    if w.is_alive():^\n",
      "^ ^ ^ ^ ^ ^ ^^ \n",
      "^AssertionError^^: ^can only test a child process^\n",
      "^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    ^\n",
      "self._shutdown_workers()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
      "\n",
      "                ^ ^^ ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^  ^ ^^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError: ^can only test a child process^\n",
      "^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    self._shutdown_workers()^\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^    ^if w.is_alive():^\n",
      "^ ^ ^\n",
      " AssertionError :  can only test a child process\n",
      "  ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "^Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^^    ^^self._shutdown_workers()^^\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "if w.is_alive():\n",
      "                ^  ^^^^^^^^^^^^^^^Exception ignored in: ^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "\n",
      "^ ^     ^self._shutdown_workers() ^\n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^      ^ if w.is_alive():\n",
      "^ ^  ^  ^  ^^^^ ^^ ^^^ ^ ^^^^^^^^^^^^^^^^\n",
      "^^AssertionError^^: ^^^can only test a child process^\n",
      "^^^^Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^Traceback (most recent call last):\n",
      "      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      " ^self._shutdown_workers() \n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^ ^      ^if w.is_alive():^ \n",
      "^  ^  ^   ^ ^ ^^ ^^ ^\n",
      " ^^AssertionError^^: ^^can only test a child process^^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^  ^^ ^ ^  ^ ^^^ ^^   ^\n",
      "AssertionError^^: ^^can only test a child process^^\n",
      "Exception ignored in: ^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^        self._shutdown_workers()^self._shutdown_workers()\n",
      "\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^        ^if w.is_alive():if w.is_alive():^\n",
      "\n",
      "^  ^ ^^  ^ ^^ ^  ^   ^^^ ^^ ^^^\n",
      "^^^AssertionError^^: ^^can only test a child process^^\n",
      "^^^^^^^^Exception ignored in: ^^^\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "\n",
      " self._shutdown_workers()  \n",
      "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "          if w.is_alive(): \n",
      "            ^^  ^  ^^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^\n",
      " ^ ^ ^ ^^^ ^^ ^^ ^ ^ ^^ ^^ ^^^^^^^^^^^\n",
      "AssertionError^^\n",
      "^: AssertionError^can only test a child process: \n",
      "^^can only test a child process\n",
      "^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^Exception ignored in: \n",
      "^Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "Traceback (most recent call last):\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    ^    self._shutdown_workers()^self._shutdown_workers()^\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "        ^if w.is_alive():if w.is_alive():\n",
      "^\n",
      "^   ^ ^ ^ ^^  ^     ^^^ ^\n",
      "AssertionError^^: ^^can only test a child process\n",
      "^^^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^^\n",
      "^^Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^^    self._shutdown_workers()^\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "      File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():    assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "\n",
      "                      ^   ^   ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^\n",
      " ^^ ^^  ^^ ^^^  ^^^ ^^^^ ^^ ^ ^^^^^^^^^^^^^^^\n",
      "AssertionError^^: ^^can only test a child process^^^\n",
      "^^^^^^Exception ignored in: ^\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "AssertionError^Traceback (most recent call last):\n",
      ": ^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^can only test a child process\n",
      "^    ^self._shutdown_workers()^Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^    \n",
      "^Traceback (most recent call last):\n",
      "^if w.is_alive():  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^\n",
      "     ^self._shutdown_workers()\n",
      "^ ^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      " ^    ^ if w.is_alive():^ \n",
      "^  ^  ^\n",
      "  ^AssertionError: ^ ^^can only test a child process  \n",
      "^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      " ^ ^\n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "     assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
      "            ^ ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^^: can only test a child process^\n",
      "^\n",
      "Exception ignored in: AssertionError<function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>: \n",
      "can only test a child processTraceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "        self._shutdown_workers()if w.is_alive():\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "\n",
      "     if w.is_alive():\n",
      "            ^ ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "                    ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bbf983ffa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05 | loss 0.0492 | AUROC 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06 | loss 0.0423 | AUROC 0.995\n",
      "  ↳ saved new best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07 | loss 0.0744 | AUROC 0.992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08 | loss 0.0467 | AUROC 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09 | loss 0.0425 | AUROC 0.993\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt     = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "scaler  = torch.amp.GradScaler('cuda' if device=='cuda' else 'cpu')\n",
    "auroc   = tm.AUROC(task=\"binary\").to(device)\n",
    "\n",
    "best_auc = 0.0\n",
    "for epoch in range(10):\n",
    "    model.train(); \n",
    "    auroc.reset(); \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for x, y in tqdm(train_dl, leave=False):\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu'):\n",
    "            logits = model(x).squeeze()\n",
    "            loss   = loss_fn(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        auroc.update(torch.sigmoid(logits.detach()), y)\n",
    "    epoch_loss = running_loss / len(train_dl.dataset)\n",
    "    epoch_auc  = auroc.compute().item()\n",
    "    print(f\"epoch {epoch:02d} | loss {epoch_loss:.4f} | AUROC {epoch_auc:.3f}\")\n",
    "    if epoch_auc > best_auc:\n",
    "        best_auc = epoch_auc\n",
    "        torch.save(model.state_dict(), \"cnn_baseline.pt\")\n",
    "        print(\"  ↳ saved new best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:10:40.507071Z",
     "iopub.status.busy": "2025-08-27T17:10:40.506345Z",
     "iopub.status.idle": "2025-08-27T17:10:40.670301Z",
     "shell.execute_reply": "2025-08-27T17:10:40.669571Z",
     "shell.execute_reply.started": "2025-08-27T17:10:40.507043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved? True size MB: 132.984826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"saved?\", os.path.exists(\"cnn_baseline.pt\"),\n",
    "      \"size MB:\", os.path.getsize(\"cnn_baseline.pt\")/1e6 if os.path.exists(\"cnn_baseline.pt\") else 0)\n",
    "\n",
    "# reload to be sure the state_dict is good\n",
    "state = torch.load(\"cnn_baseline.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to create a scan-grouped train/validation split using GroupShuffleSplit so that patches from the same seriesuid don’t leak across splits, then rebuild matching negative samples per split, and finally construct LunaPatchDS datasets and DataLoaders (with augmentation for train, none for val) to feed the model./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:11:33.605319Z",
     "iopub.status.busy": "2025-08-27T17:11:33.605015Z",
     "iopub.status.idle": "2025-08-27T17:11:34.426285Z",
     "shell.execute_reply": "2025-08-27T17:11:34.425718Z",
     "shell.execute_reply.started": "2025-08-27T17:11:33.605294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "groups = patch_df['seriesuid'].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(patch_df, groups=groups))\n",
    "\n",
    "pos_train = patch_df.iloc[train_idx].reset_index(drop=True)\n",
    "pos_val   = patch_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# rebuild negatives for each split\n",
    "def make_negs(pos_df):\n",
    "    series = pos_df['seriesuid'].unique()\n",
    "    return (pd.DataFrame({'seriesuid': np.random.choice(series, size=len(pos_df))})\n",
    "            .assign(patch_file=lambda d: d.seriesuid.map(\n",
    "                lambda s: pos_df[pos_df.seriesuid==s]\n",
    "                          .sample(1, random_state=0).patch_file.values[0]))\n",
    "            .assign(label=0))\n",
    "\n",
    "neg_train, neg_val = make_negs(pos_train), make_negs(pos_val)\n",
    "\n",
    "train_ds = LunaPatchDS(pos_train, neg_train, PATCH_DIR, augment=True)\n",
    "val_ds   = LunaPatchDS(pos_val,   neg_val,   PATCH_DIR, augment=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to evaluate on the validation set by switching the model to eval mode, collecting probabilities with torch.no_grad(), and computing the AUROC using torchmetrics, then printing the final grouped validation AUROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:11:41.405705Z",
     "iopub.status.busy": "2025-08-27T17:11:41.405112Z",
     "iopub.status.idle": "2025-08-27T17:13:14.049572Z",
     "shell.execute_reply": "2025-08-27T17:13:14.048783Z",
     "shell.execute_reply.started": "2025-08-27T17:11:41.405678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID AUROC (scan-grouped): 0.986\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "auroc = tm.AUROC(task=\"binary\")\n",
    "y_true, y_prob = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_dl:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        prob = torch.sigmoid(model(xb).squeeze())\n",
    "        y_true.append(yb.cpu()); y_prob.append(prob.cpu())\n",
    "y_true = torch.cat(y_true); y_prob = torch.cat(y_prob)\n",
    "val_auc = auroc(y_prob, y_true).item()\n",
    "\n",
    "print(f\"VALID AUROC (scan-grouped): {val_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to report the validation AUROC and compute threshold-based metrics at thr=0.50 (precision, recall, F1). The AUROC of 0.986 shows the model ranks positives above negatives very well, but at a fixed 0.5 threshold it predicted no positives, so precision/recall/F1 are 0.0 and sklearn warns about “no predicted samples”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T17:13:46.616508Z",
     "iopub.status.busy": "2025-08-27T17:13:46.616208Z",
     "iopub.status.idle": "2025-08-27T17:13:46.636154Z",
     "shell.execute_reply": "2025-08-27T17:13:46.635331Z",
     "shell.execute_reply.started": "2025-08-27T17:13:46.616483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.50  Precision=0.000 Recall=0.000 F1=0.000  AUROC=0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "thr = 0.5\n",
    "y_pred = (y_prob.numpy() >= thr).astype(np.int32)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_true.numpy(), y_pred, average='binary')\n",
    "\n",
    "print(f\"thr={thr:.2f}  Precision={p:.3f} Recall={r:.3f} F1={f1:.3f}  AUROC={val_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion. The baseline CNN separates classes strongly (high AUROC) but is miscalibrated for the 0.5 cutoff—likely due to class imbalance. Next steps: pick a better decision threshold (e.g., maximize F1 or Youden’s J on the val set), inspect the precision–recall curve, and consider class weighting/pos_weight (or focal loss) and probability calibration (temperature scaling or Platt scaling). This will convert the strong ranking into usable operating points for your task."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8137154,
     "sourceId": 12864260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
