{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 · CNN Baseline (Real Negatives)\n",
    "**Purpose** This notebook revisits the baseline 3D CNN but replaces the previous synthetic noise negatives with real background patches extracted from patient scans (far from annotated nodules). This makes the task more realistic and provides a stronger, fairer baseline.\n",
    "\n",
    "Key steps include:\n",
    "- Building real negative samples per scan using distance constraints from nodule centers\n",
    "- Recreating grouped train/validation splits (by seriesuid) to prevent leakage\n",
    "- Training the same lightweight 3D CNN architecture for a like-for-like comparison \n",
    "- Evaluating with AUROC and threshold metrics, and comparing against the synthetic-negatives baseline\n",
    "\n",
    "This iteration aims to reduce optimism in the baseline and better reflect true deployment conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to import the required libraries, set the compute device, define paths for the positive (nodule) and real negative background patches, load their indices (patch_index.csv and bg_index.csv) while assigning labels (1 for positives, 0 for negatives), and print the resulting DataFrame shapes to confirm the data has been mounted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.video import r3d_18\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-11T11:26:42.955938Z",
     "iopub.status.busy": "2025-10-11T11:26:42.955629Z",
     "iopub.status.idle": "2025-10-11T11:26:55.845565Z",
     "shell.execute_reply": "2025-10-11T11:26:55.844856Z",
     "shell.execute_reply.started": "2025-10-11T11:26:42.955913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186, 7) (601, 8)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "POS_DATA = Path(\"/kaggle/input/patches\")                               # patches_64mm + patch_index.csv\n",
    "NEG_DATA = Path(\"/kaggle/input/negative-mining-output-data\")           # bg_patches_64mm + bg_index.csv\n",
    "\n",
    "POS_DIR = POS_DATA/\"patches_64mm\"\n",
    "NEG_DIR = NEG_DATA/\"bg_patches_64mm\"\n",
    "\n",
    "pos_df = pd.read_csv(POS_DATA/\"patch_index.csv\").assign(label=1)\n",
    "neg_df = pd.read_csv(NEG_DATA/\"bg_index.csv\").assign(label=0)\n",
    "\n",
    "print(pos_df.shape, neg_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to create a grouped train/validation split using GroupShuffleSplit on positives by seriesuid (to avoid patient-level leakage), then align real negatives to each split by filtering neg_df to the same seriesuid sets. Finally, we print the shapes to verify that positives and negatives are correctly partitioned for both train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T11:28:09.506612Z",
     "iopub.status.busy": "2025-10-11T11:28:09.506046Z",
     "iopub.status.idle": "2025-10-11T11:28:09.525998Z",
     "shell.execute_reply": "2025-10-11T11:28:09.525316Z",
     "shell.execute_reply.started": "2025-10-11T11:28:09.506584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938, 7) (480, 8) | (248, 7) (121, 8)\n"
     ]
    }
   ],
   "source": [
    "groups = pos_df['seriesuid'].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, va_idx = next(gss.split(pos_df, groups=groups))\n",
    "pos_tr, pos_va = pos_df.iloc[tr_idx].reset_index(drop=True), pos_df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "# pair negatives to the same split by seriesuid\n",
    "neg_tr = neg_df[neg_df.seriesuid.isin(pos_tr.seriesuid.unique())].reset_index(drop=True)\n",
    "neg_va = neg_df[neg_df.seriesuid.isin(pos_va.seriesuid.unique())].reset_index(drop=True)\n",
    "\n",
    "print(pos_tr.shape, neg_tr.shape, \"|\", pos_va.shape, neg_va.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to define a dataset RealNegDS that loads nodule patches from POS_DIR and real background patches from NEG_DIR, normalizes shape to 64³, and applies light augmentation to positives. We then instantiate train/val datasets and wrap them in DataLoaders (shuffled train, non-shuffled val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T11:28:10.925360Z",
     "iopub.status.busy": "2025-10-11T11:28:10.925046Z",
     "iopub.status.idle": "2025-10-11T11:28:10.942390Z",
     "shell.execute_reply": "2025-10-11T11:28:10.941646Z",
     "shell.execute_reply.started": "2025-10-11T11:28:10.925336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_64(c):\n",
    "    if c.shape == (64,64,64): return c.astype(np.float32, copy=False)\n",
    "    # centre-crop/pad per axis\n",
    "    def fix(a, ax, tgt=64):\n",
    "        s=a.shape[ax]\n",
    "        if s>=tgt:\n",
    "            st=(s-tgt)//2; sl=[slice(None)]*3; sl[ax]=slice(st,st+tgt); return a[tuple(sl)]\n",
    "        b=(tgt-s)//2; a2=tgt-s-b; pad=[(0,0)]*3; pad[ax]=(b,a2); return np.pad(a,pad)\n",
    "    c=fix(fix(fix(c,0),1),2); return c.astype(np.float32, copy=False)\n",
    "\n",
    "class RealNegDS(Dataset):\n",
    "    def __init__(self, pos, neg, pos_dir, neg_dir, augment=True):\n",
    "        self.df = pd.concat([pos, neg]).sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "        self.pos_dir, self.neg_dir, self.aug = pos_dir, neg_dir, augment\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        pth = (self.pos_dir if r.label==1 else self.neg_dir) / r.patch_file\n",
    "        cube = np.load(pth); cube = to_64(cube)\n",
    "        if self.aug and r.label==1:\n",
    "            if random.random()<.5: cube = cube[::-1]\n",
    "            if random.random()<.5: cube = np.rot90(cube, 1, (1,2))\n",
    "        cube = np.ascontiguousarray(cube)\n",
    "        x = torch.from_numpy(cube).float().unsqueeze(0)\n",
    "        y = torch.tensor(r.label, dtype=torch.float32)\n",
    "        return x,y\n",
    "\n",
    "train_ds = RealNegDS(pos_tr, neg_tr, POS_DIR, NEG_DIR, augment=True)\n",
    "val_ds   = RealNegDS(pos_va, neg_va, POS_DIR, NEG_DIR, augment=False)\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to build the 3D ResNet-18 baseline adapted to single-channel input, set up BCEWithLogitsLoss, AdamW, and AUROC, and run an 8-epoch train/validate loop. We log loss and AUROC each epoch and save a checkpoint (cnn_baseline_realnegs.pt) whenever validation AUROC improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00 | train loss 0.6488 auc 0.531 | val loss 0.6390 auc 0.190\n",
      "  ↳ saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train loss 0.4748 auc 0.792 | val loss 0.7693 auc 0.818\n",
      "  ↳ saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 | train loss 0.4550 auc 0.808 | val loss 0.7360 auc 0.150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03 | train loss 0.4219 auc 0.824 | val loss 0.3448 auc 0.890\n",
      "  ↳ saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04 | train loss 0.4207 auc 0.841 | val loss 6.7381 auc 0.818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05 | train loss 0.4616 auc 0.808 | val loss 0.6343 auc 0.785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06 | train loss 0.4253 auc 0.833 | val loss 2.5729 auc 0.821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07 | train loss 0.4133 auc 0.834 | val loss 4.5833 auc 0.820\n"
     ]
    }
   ],
   "source": [
    "model = r3d_18(weights=None)\n",
    "model.stem[0] = nn.Conv3d(1,64,7,2,3,bias=False)\n",
    "model.fc = nn.Sequential(nn.Linear(512,128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128,1))\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt     = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "auroc   = tm.AUROC(task=\"binary\").to(device)\n",
    "\n",
    "best_auc=0.0\n",
    "for epoch in range(8):\n",
    "    # train\n",
    "    model.train(); auroc.reset(); run_loss=0.0\n",
    "    for xb,yb in tqdm(train_dl, leave=False):\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb).squeeze()\n",
    "        loss   = loss_fn(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        run_loss += loss.item()*xb.size(0)\n",
    "        auroc.update(torch.sigmoid(logits).detach(), yb)\n",
    "    tr_loss = run_loss/len(train_ds); tr_auc = auroc.compute().item()\n",
    "    # validate\n",
    "    model.eval(); auroc.reset(); run_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in val_dl:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            lg = model(xb).squeeze(); l = loss_fn(lg, yb)\n",
    "            run_loss += l.item()*xb.size(0)\n",
    "            auroc.update(torch.sigmoid(lg), yb)\n",
    "    va_loss = run_loss/len(val_ds); va_auc = auroc.compute().item()\n",
    "    print(f\"epoch {epoch:02d} | train loss {tr_loss:.4f} auc {tr_auc:.3f} | val loss {va_loss:.4f} auc {va_auc:.3f}\")\n",
    "    if va_auc>best_auc:\n",
    "        best_auc=va_auc; torch.save(model.state_dict(),\"cnn_baseline_realnegs.pt\"); print(\"  ↳ saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion. Replacing synthetic noise with real background negatives yields a more realistic and stricter baseline. You should expect AUROC to drop slightly vs. the synthetic-negatives model (harder negatives), but metrics are now more trustworthy for downstream comparisons (e.g., hard negative mining, focal loss, better augmentations). Next steps: (1) calibrate a decision threshold on the val set, (2) inspect error cases—especially false positives on difficult background, and (3) consider curriculum or hard-negative sampling to further toughen training while keeping evaluation strictly grouped by seriesuid."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 982666,
     "sourceId": 1659908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8454481,
     "sourceId": 13334106,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8137154,
     "sourceId": 12864260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
